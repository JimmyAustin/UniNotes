\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}

\title{Brief Article}
\author{The Author}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\tableofcontents
\section{Lecture 1: Introduction to Artificial Intelligence}

\subsection{What is Intelligence?}

Something is intelligent if it can communicate, it has internal knowledge, it has world knowledge, it has intentions and plans to fulfil these intentions and it has creativity. AI is the study of mental faculties through the use of computational models. The goals of AI practitioners is to find out about the nature of intelligence and to build an intelligent machine, that is, to build a machine that thinks like a human (thinks rationally) and acts like a human (acts rationally).
\\\\
In 1950, Turing suggested that the major components of AI are knowledge, reasoning, language understanding and learning. 

\subsection{Rational Behaviour}
Rational Behaviour is doing the right thing, that which is expected to maximise goal achievement, given the available information.

\subsection{Rational Agents}
A agent is an entity that perceives and acts. In abstract form, an agent is a function that maps percept histories to actions: $f: P \rightarrow A$. For any given class of environments and tasks, we seek the agent (or class of agents) with the best performance. There is a caveat though. Computational limitations make perfect rationality unachievable. Therefore we must design the best program for a given machine's resources.
\subsection{Autonomous Agency}
Autonomy is the ability to operate independently, while agency is a internal goal structure and external behaviour which generally serves to satisfy a goal structure. The requirements of autonomous agency are:
\begin{itemize}
\item Pragmatics
\item Generalisation and specialisation
\item Incremental learning
\item Goal-driven learning
\item Defeasibility - open in principle to revision, valid objection, forfeiture, or annulment.
\item Uncertainty
\end{itemize}

\section{Lecture 2: Intelligent Agents}

\subsection{Agents}

An agent is anything that can be viewed as perceiving its environment through sensors and acting upon the environment through actuators. For a human agent, sensors would be our eyes or ears, while our actuators would be our hands, legs, mouth and other body parts. For a robot they would be cameras and motors, respectively.
\\
An agent function maps from percept histories to actions: $f: P \rightarrow A$\\
The agent program runs on the physical architecture to produce this function.\\
The agent itself is a combination of the architecture and the program.\\

\subsection{Rationality and Rational Agents}

Rationality depends on a performance measure, the agent's prior knowledge of the environment, the actions that the agent can perform and the percept sequence to data. The formal definition is: 

\begin{verbatim}
For each possible percept sequence, a tonal agent should select an action
that is expected to maximise its performance measure, given the evidence
provided by the percept sequence and the agent's built-in knowledge.
\end{verbatim}
The agent is considered autonomous if its behaviour is determined by its own experience. 

\subsection{Task Environment}

To design a rational argument, you have to specify a task environment. This is made by PEAS:
\begin{itemize}
\item Performance Measure
\item Environment
\item Actuators
\item Sensors
\end{itemize}

\subsubsection{Example - Automated Taxi Driver}
\begin{itemize}
\item Performance Measure - Safe, fast, legal, comfortable trip, minimise fuel consumption, maximise profits.
\item Environment - Road types, road contents, customers, operating conditions.
\item Actuators - Control over the car, communication with other vehicles and passengers.
\item Sensors - Cameras, sonar, speedometer, GPS, odometer, engine sensors, speech recognizer.
\end{itemize}

\subsubsection{Example - Internet Shopping Agent}
\begin{itemize}
\item Performance Measure - Cheap, good quality, appropriate product.
\item Environment - Current WWW sites, vendors.
\item Actuators - Display to user, follow URL, fill in form.
\item Sensors - HTML pages (text, graphics, scripts)
\end{itemize}

\subsection{Environment Types}
The environment type largely determines the agent design.
\begin{itemize}
\item Fully observable vs Partially obserable- An agent's sensors give it access to the complete state of the environment at all times.
\item Known vs Unknown - An agent knows the "laws" of the environment.
\item Single vs Multi Agent - An agent operating by itself in an argument
\item Deterministic vs Stochastic - The next state is completely determined by the current state and the action executed by the agent.
\item Episodic vs Sequential - The agent's experience is divided into atomic episodes. The next episode does not depend on previous actions. In each episode the agent precepts a percept and performs a single action.
\item Static vs Dynamic - The agent is unchanged while an agent is deliberating.
\item Discrete vs Continuous) - Pertains to number of states, the way time is handled, and number of percepts and actions. E.G, state may be continuous, but actions may be discrete.
\end{itemize}
\subsection{Agent Types}
The agent type is based on how actions are selected.
\begin{itemize}
\item Simple reflex - current percept
\item Model based - + internal state
\item Goal based - + goal
\item Utility based - + utility function
\item Learning
\end{itemize}

\subsection{How components of agent programs work?}
Depends on the representations of states:
\begin{itemize}
\item Atomic - each state is indivisible (Search game playing, Markov Decision Processes)
\item Factored - splits each state into attributes, each of which has a value (Propositional logic, Planning, Bayesian Networks, Machine Learning).
\item Structured - represents how things are related to each other (First order logic, Bayesian networks, Semantic networks).
\end{itemize}

\section{Lecture 3-7: Problem Solving As Search - ONGOING}

The objectives for problem solving include:
\begin{itemize}
\item Problem formulation
\item Control strategies
\begin{itemize}
\item Tentative
\begin{itemize}
\item Uninformed
\begin{itemize}
\item Backtrack
\item Tree and Graph Search
\end{itemize}
\item Informed
\begin{itemize}
\item Best-first (greedy) search
\item A
\item A*
\end{itemize}
\end{itemize}
\item Irrevocable
\begin{itemize}
\item Informed
\begin{itemize}
\item Hill climbing
\item Greedy search
\item Local beam search
\item Simulated annealing
\item Genetic algorithms
\end{itemize}
\end{itemize}

\end{itemize}
\item Adversarial search
\begin{itemize}
\item Optimal decisions
\item $\alpha - \beta$ pruning
\item Imperfect, real-time decisions
\end{itemize}
\end{itemize}
\subsection{A problem-solving agent}
\begin{verbatim}
Function Simple-Problem-Solving-Agent(percept) 
returns seq

persistent: seq - action sequence, initially null 
state - description of current world state
goal - a goal, initially null
problem - a problem formulation 

state <- UpdateState(state,percept)
goal <- FormulateGoal(state)
problem <- FormulateProblem(state,goal) 
seq <- Search(problem)
return seq
\end{verbatim}
\subsubsection{Example - Romania}
The problem is: On holiday in romania; currently in Arad. Flight leaves tomorrow from Bucharest.
The formualate goal is to be in Bucharest, and the problem includes the states (various cities) and actions (driving between these cities). The way to find the solution is to find the sequence of cities that you can cross between.
\subsection{Problem Formulation}
The basic constituents of problem formulation are the states, goals, actions and constraints.\\
The state space is the set of all states reachable from the initial state by any sequence of actions. The path in the state space is any sequence of actions leading from one state to another. 
\\
To represent a problem you need
\begin{itemize}
\item An initial state
\item Operators (actions) and a transition model
\item Constraints
\item A goal test
\item A path cost function
\end{itemize}

For the romainian example given above, this would correspond to:
\begin{itemize}
\item An initial state - "at Arad"
\item Operators (actions) {Go(Sibiu, Go(Timisoara),..} and a transition model (Result(In(Arad),Go(Zerind))->In(Zerind)
\item Constraints - None
\item A goal test - In(Bucharest)
\item A path cost function - The sum of all distances, or the number of actions executed.
\end{itemize}

For an algorithm that governs the assembly of an object using a robot, they might be
\begin{itemize}
\item States - Real valued coordinates of robot joint angles
\item Actions - Motions by the robot
\item Constraints - Arm cannot fully rotate in certain angles
\item Goal test - Complete assembly
\item Path cost - time to execute
\end{itemize}
\subsection{Selecting a State Space}

The real world is complex. This means that the state space must be abstracted for problem solving. An abstract state may equal a set of real states (being In(Zerind) might mean you are in one of a few different hotels in Zerind). In this way, an abstract action can be a complex combination of real actions. A trip from one city to another represents many individual actions. A abstract solution is a solution that can be expanded into a set of paths in the real world. Each abstract action should be easier to perform than solving the original problem.

\subsection{Control Strategies}
Control strategies can be separated by two defining characteristics, there tentativeness (irrevocable/tentative) and informedness (uninformed/informed).

\subsection{Tentative Control Strategies}
\subsubsection{Backtracking}
In backtracking we keep one path at a time only. If we fail, we go back to the last decision point and erase the failed path. Backtracking occurs when we generate a previously encountered state description, we pass an arbitrary number of rules without reaching our goal, or there are no more applicable rules.
\subsubsection{Graph Search}
We keep track of several paths simultaneously. This is done using a structure called a search tree/graph. A graph is a set of nodes, while arcs connect certain pairs of nodes (in a directed graph these arcs are one way). If a node $n_i$ is accessible from $n_k$, then there is a path between the two nodes. If you expand a node, you are finding all of its children. 
\\
Using a search tree, the root is the initial state, while it's children are the nodes accessible from it. Leaves are states without visible successor. At each iteration, pick a leaf node and expand it.
\subsection{Search Strategies}
A search strategy is defined by picking the order of node expansion. These strategies are evaluated along several dimensions.
\begin{itemize}
\item Completeness: Does it always find a solution if one exists?
\item Time complexity: Number of nodes generated
\item Space complexity: Maximum number of nodes in memory
\item Optimality: Does it always find a least-cost solution.
\end{itemize}

Time and space complexity are measured in terms of:
\begin{itemize}
\item b: maximum branching factor of the search tree
\item d: depth of the least-cost solution
\item m: maximum depth of any path in the state space (may be infinite)
\end{itemize}

\subsection{Uninformed search strategies}
These search strategies use only the information available in the problem definition
\subsubsection{Breadth-first search (BFS)}
Expand the shallowest unexpanded node. Uses a large amount of space.
\begin{itemize}
\item Implementation: FIFO queue. Put successors at the end of the queue.
\item Complete: Yes (if b is finite)
\item Time: $O(b^d)$
\item Space: $O(b^d)$ (keeps every node in memory) 
\item Optimal: Yes (if all actions have the same cost)
\end{itemize}

\subsubsection{Uniform-cost search (UFS)}
Expand the least-cost unexpanded node.
\begin{itemize}
\item Implementation: insert in order of increasing path cost.(The same as BFS if all steps cost equal).
\item Complete: Yes, if all steps cost >= 0
\item Time: $O(b_{1+floor(C*0)})$, C is the cost of the optimal solution.
\item Space: $O(b_{1+floor(C*0)})$
\item Optimal: Yes - nodes expanded in increasing order of g(n) = cost of path to node n
\end{itemize}

\subsubsection{Depth-first search}
Expand deepest unexpanded node
\begin{itemize}
\item Implementation - LIFO - insert successors in front of queue.
\item Complete - Infinite-state spaces: No, Finite-state spaces: Yes
\item Time - $O(b^m)$, terrible if m is much larger than d
\item Space - $O(bm)$
\item Optimal - No
\end{itemize}
\subsubsection{Iterative Deepening Search}
Check all nodes of D=0, then D=1, then D=2.
\begin{itemize}
\item Complete: Yes
\item Time: $O(b^d)$
\item Space: $O(bd)$
\item Optimal? Yes, if step costs are identical.
\end{itemize}

\subsection{Informed Search Strategies: Best-first Search}

Heuristic Graph search procedures use heuristic information to help reduce the search. This involves the use of an evaluation functions - a real valued function used to compute the "promise" of a node.

\subsubsection{Definitions}

\begin{itemize}
\item $k(n_i,n_j)$ - actual cost of minimal cost path between $n_i$ and $n_j$
\item $h*(n)$ =$ min\{k(n,t_j)\}$ - Minimum of all the $k(n,t_j)$ over the entire set of nodes $\{t_j\}$
\item $g*(n)$ = $k(s,n)$ - Minimum cost from the start node $s$ to $n$
\item $f*(n)=g*(n)+h*(n)$ - Cost of an optimal path constrained to go through $n$
\item $f*(s)=h*(s)$ - Cost of an unconstrained optimal path from s to goal.
\item $f(n)$ - estimate of the minimal cost path constrained to go through node $n$
\item $g(n)$ - estimate of $g*(n)$. Usual choice is the cost of the path in the search tree from $s$ to $n \rightarrow g(n) \geq g*(n)$ 
\item $h(n)$ - heuristic function.
\end{itemize}

\subsection{Greedy Best-First Search}

\begin{itemize}
\item Description: Expands the node that is closest to the goal. $f(n) = h(n)$
\item Example: $h_{SLD}(n)$ = Straight-Line Distance to the goal.
\item Complete: In infinite spaces, no, in finite state spaces, Yes.
\item Time: $O(b^m)$
\item Space: $O(b^m)$
\item Optimal: No
\end{itemize}

\subsection{A}

\begin{itemize}
\item Description: Graphsearch using the evaluation function $f(n) = g(n) + h(n)$. It expands the next node on the fronteir with the smallest value of f(n).
\item Complete: Yes
\item Time: ?
\item Space: $O(b^d)$
\item Optimal: No
\end{itemize}

\subsection{A*}
See ProblemSolving Pg66
\begin{itemize}
\item Description: If $\forall n h(n) \leq h*(n)$
\item Complete: Yes
\item Time: $O(b^{max|h-h*|})$
\item Space: $O(b^d)$
\item Optimal: Yes
\end{itemize}

\subsection{Problems}

A problem with few restrictions on actions is called a relaxed problem. The cost of an optimal solution to a relaxed problem is an admissable heuristic for the original problem.

\subsection{Irrevocable Search Algorithms}

In many optimization problems, the goal state is the solution and the state space is a set of "complete configurations". To find configurations that satisfy constraints, we use local search algorithms that keep a single "current" state and then try to improve it.

\subsection{Hill Climbing}

Hill Climbing continually selects the best possible next state. This has the issue of hitting a local maxima.

\subsection{Local Beam Search}
Local Beam Search generates k randomly generated states, then at each iteration all the successors of all k states are generated. If any of these are valid goal states, return it. If not, select the top k best successors from the complete list and repeat.
\subsection{Simulated Annealing}

Simulated Annealing escapes local maxima by allowing some bad moves but gradually decreasing their frequency. Involves both a Temperature (T), and an Annealing schedule (the rate at which the temperature is lowered). See Pg 86. One can rpove that if T decreases slowly enough, then simulated annealing search will find a global optimum with probability approaching 1. It is widely use in VLSI layout and airline scheduling.

\subsection{Genetic Algorithms}

A successor state is generated by combining two parent states. This involves starting with a population of k randomly generated states. A state is represented as a string over a finite alphabet of genes (often a string of 0s and 1s). A fitness function is used to produce the next generation of states by selection, crossover and mutation.

\subsection{Adversarial Search Algorithms}

The following section covers two person, perfect information games. The two players are named MAX and MIN. A position favourable to Max has a value > 0, while a position favourable to Min has a value < 0. The goal is to find a winning strategy for Max. For all nodes representing a game situation where it is MIN's move next, show that Max can win from every position to which Min might move or show that Max can win from just one position to which Max might move.
\\
This has problems. You must specifiy a move for every possible opponent reply. This can be difficult with an unpredictable opponent. Time limits are also an issue. Not all games can be searched to the end.
\subsubsection{MinMax Ideas}

You have to choose the move with the highest minimax value: best achievable payoff against the best play. This is complete, optimal, with a time complexity of $O(b^m)$ and a space complexity of O(bm).

For chess, b is roughly 35, and m = 1000. An exact solution is completely infeasible.

\subsection{Resource Limits}

In order to cover the most ground with given resource limits. The standard approach is a cutoff test (limit to a depth limit). We can use an evaluation function that estimates the desirability of a position (number of white queens vs black queens for chess). We can also use forward pruning (only look at the n-best moves).

\subsection{$\alpha - \beta$ Procedure}

An 

\section{Lecture 8: Knowledge Representation - INCOMPLETE}
\section{Lecture 11: Probability - INCOMPLETE}
\section{Lecture 12: Bayesian Networks - INCOMPLETE}
\section{Lecture 13-14: Bayesian Networks II - INCOMPLETE}
\section{Lecture 15-16: MDPs - INCOMPLETE}
\section{Lecture 17-18: RL - INCOMPLETE}
\section{Lecture 19-20:Mathematical Principles of ML - Decision Trees - INCOMPLETE}
\section{Lectures 21-22: Supervised Learning - INCOMPLETE}

\end{document}  